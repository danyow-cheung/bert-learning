# BERT Variants II - Based on Knowledge Distillation
we transfer knowledge from a large pre-trained BERT to a small BERT using knowledge distillation. 
我們使用知識蒸餾將知識從大型預訓練 BERT 轉移到小型 BERT。





We use the following methods for performing task-agnostic data augmentation:
我們使用以下方法來執行與任務無關的數據擴充：
- Masking 掩碼
- POS-guided word replacement 詞性引導詞替換
- n-gram sampling n 元抽樣

> 有點無言，一整章都是算法
> 
